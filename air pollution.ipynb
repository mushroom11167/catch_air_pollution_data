{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **空氣汙染**\n",
    "資料來源自[環境部環境資料開放平臺](<https://data.moenv.gov.tw/>)，先至網站註冊會員得到API，再點選開發指南至[資料擷取API線上說明文件](<https://data.moenv.gov.tw/swagger/>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package\n",
    "先安裝我們要使用的套件，分別為requests、pandas、datetime及time：\n",
    "* request：抓取網路資料，對網站發出請求\n",
    "* pandas：處理資料及csv文件\n",
    "* datetime：處理日期\n",
    "* time：每次請求間格的時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import pandas as pd \n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爬蟲\n",
    "### crawl_and_append_csv\n",
    "這步是要從我們指定的網站去下載csv檔，併進空的資料集裡\n",
    "* combined_df 存放每次下載資料的資料集\n",
    "* pd.read_csv 讀取csv檔的資料\n",
    "* pd.concat 將每次下載的資料合併\n",
    "\n",
    "### date_range\n",
    "這步是抓取時間內的資料，由於每次下載上限是1000筆，所以是分批下載，最後將所有檔案合併成一個CSV檔\n",
    "* start_date、end_date：爬取資料的開始和結束日期\n",
    "* api_key：API金鑰\n",
    "* limit：每次爬取資料的筆數（最大1000筆）\n",
    "* output_path：輸出的路徑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_and_append_csv(url, combined_df):\n",
    "    res = requests.get(url)\n",
    "    data_csv = pd.read_csv(url)\n",
    "    \n",
    "    combined_df = pd.concat([combined_df, data_csv], ignore_index=True)\n",
    "    return combined_df, len(data_csv)\n",
    "\n",
    "def date_range(start_date, end_date, url_template, api_key, limit, output_path):\n",
    "    start_date_str = datetime.strptime(start_date, \"%Y-%m-%d\").strftime(\"%Y-%m-%d\")\n",
    "    end_date_str = datetime.strptime(end_date, \"%Y-%m-%d\").strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    combined_df = pd.DataFrame() \n",
    "    \n",
    "    offset = 0\n",
    "    while True:\n",
    "        url = url_template.format(start_date_str, end_date_str, api_key, offset, limit)\n",
    "        combined_df, current_batch_size = crawl_and_append_csv(url, combined_df)\n",
    "        \n",
    "        # 檢查抓取的資料數量，如果少於 limit，則說明已經抓完，跳出循環\n",
    "        if current_batch_size < limit:\n",
    "            break\n",
    "        \n",
    "        # 更新 offset\n",
    "        offset += limit\n",
    "        time.sleep(1)  \n",
    "    \n",
    "    # 將合併後的資料寫入 CSV 檔案\n",
    "    combined_df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "\n",
    "# 定義起始和結束日期\n",
    "start_date = input(\"請輸入開始日期（格式：YYYY-MM-DD）：\")\n",
    "end_date = input(\"請輸入結束日期（格式：YYYY-MM-DD）：\")\n",
    "# api 金鑰\n",
    "api_key = \"api\"\n",
    "# limit 是每次下載的筆數，一次最多只能下載 1000 筆\n",
    "limit = 1000\n",
    "# 存放路徑與檔名\n",
    "output_path = \"D:/mushroom/research/air/2024/0318_0324/2022_Air.csv\"\n",
    "# URL 模板\n",
    "url_template = \"https://data.moenv.gov.tw/api/v2/aqx_p_13?&format=csv&filters=MonitorDate,GR,{0}|MonitorDate,LE,{1}&api_key={2}&offset={3}&limit={4}\"\n",
    "\n",
    "date_range(start_date, end_date, url_template, api_key, limit, output_path)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
